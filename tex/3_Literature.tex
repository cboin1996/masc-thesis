\chapter{Literature Review} \label{chap:litreview}
In this chapter, an in-depth summary of the current state-of-the-art research related to 
FRL and DRL applied to AV platooning is presented. 

\section{Federated Reinforcement Learning}

Reinforcement learning is often a sequential learning process, and as such data is often 
non-IID with a small sample space \cite{sutton2018reinforcement}.  HFRL provides the ability 
to aggregate experience while increasing the sample efficiency thus providing more accurate 
and stable learning  \cite{IntelAI19}. It is important to note that few existing works apply 
FRL to autonomous driving at the time of this study.  Some of the current works studying 
HFRL to a variety of applications are summarized below.

% HFRL references here %
% \cite{Ren2019}
A study by \cite{Lim2020} aims to increase the performance of RL methods applied to 
multi-IoT device systems.  RL models trained on single devices are often unable to control 
devices in a similar albeit slightly different environment \cite{Lim2020}.  Currently, 
multiple devices need to be trained using separate RL agents \cite{Lim2020}. The methods 
proposed by \cite{Lim2020} sped up the learning process by 1.5 times for a two agent system. 
In a study by \cite{Nadiger2019}, the challenges in the personalization of dialogue 
managers, smart assistants and more are explored.  RL has proven to be successful in 
practice for personalized experiences; however long learning times and no sharing of data 
limits the ability for RL to be applied at scale.  Applying HFRL to atari non-playable 
characters in pong showed a median improvement of ~17\% for the personalization time 
\cite{Nadiger2019}. Lastly, \cite{Liu2019b} discusses RL as a promising algorithm for 
smart navigation systems, with the following challenges: long training times, poor 
generalization across environments, and storing data over long periods of time.  In order 
to address these problems \cite{Liu2019b} proposed the architecture `Lifelong FRL', 
which can be categorized as a HFRL problem.  It was found the Lifelong FRL increased 
the learning rate for the smart navigation system when tested on robots in a cloud 
robotic system \cite{Liu2019b}.  

The successes of the FedAvg algorithm as a means to improve performance and training 
times for systems has inspired further research into how aggregation methods should 
be applied.  The design of the aggregation method is crucial in providing performance 
benefits to that of the base case where FRL is not applied.  The FedAvg 
\cite{BrendanMcMahan2017a} algorithm proposed the averaging of gradients in the 
aggregation method.  In contrast, \cite{Liang2019} proposed using model weights in 
the aggregation method for AV steering control. Thus, FRL applications can differ 
based upon the selection of which parameter to use in the aggregation method.  A study 
by \cite{ZhangX2020} explores applying FRL to a decentralized DRL system optimizing 
cellular vehicle-to-everything communication. \cite{ZhangX2020} utilizes the model 
weights in the aggregation method, and describes a weighting factor dividing the sum 
batch size for all agents by the training batch size for a specific agent.  In addition, 
the works of \cite{LimHyun2021} explores how FRL using gradient aggregation can improve 
convergence speed and performance on the OpenAI-gym environments CartPole-V0, 
MountainvehicleContinuous-V0, Pendulum-V0 and Acrobot-V1.  \cite{LimHyun2021} determines 
that aggregating gradients using FRL creates high performing agents for each of the 
OpenAI-gym environments relative to models trained without FRL.  In addition, 
\cite{WangXiaofei2021} applies FRL to heterogeneous edge caching.  \cite{WangXiaofei2021} 
shows the effectiveness of FRL using weight aggregation to improve hit rate, reduce 
average delays in the network and offload traffic. Lastly, \cite{Huang2021} applies FRL 
using model weight aggregation to Service Function Chains in network function virtualization 
enabled networks. \cite{Huang2021} observes that FRL using model weight aggregation 
provides benefits to convergence speed, average reward and average resource consumption. 
Below is a summary of the previously mentioned FRL works categorized by their proposed 
aggregation method.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Summary of Existing Works in the Area of FRL Categorized by FRL Aggregation Method}
    \begin{tabular}{ll} \toprule
    \textbf{FRL Aggregation Strategies} & \textbf{References} \\ \midrule
    \midrule
    Gradient Averaging & \cite{BrendanMcMahan2017a}, \cite{LimHyun2021} \\
    Model Weight Averaging & \cite{Liang2019}, \cite{ZhangX2020}, \cite{WangXiaofei2021}, \cite{Huang2021} \\ \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


Despite the differences in FRL applications within the aforementioned studies, each study 
maintain a similar goal: to improve the performance of each agent within the system.  None 
of the aforementioned works explore the differences in whether gradient or model weight 
aggregation is favourable in performance, and many of the works apply FRL to distributed 
network or communications environments.  It is the goal of this study to conclude whether 
model weight or gradient aggregation is favourable for AV platooning, as well as be one of 
the first (if not the first) to apply FRL to AV platooning.  

% Not revelant in this paper, perhaps the next it will be!
% A final important note to consider when implementing FRL is summarized from a study by \cite{Lim2020}:

% \begin{enumerate}
%     \item increasing the number of agents has a positive effect on FRL's performance,
%     \item transferring models which have completed training accelerates the training process for models in the early stages of training.
% \end{enumerate}

\section{Deep Reinforcement Learning applied to AV Platooning} \label{sec:avRL}

In order to improve the limitations of vehicle following models, DRL has been a 
steady area of research in the AV community with many authors contributing works 
to DRL applied to CACC \cite{Lin2019, Song2020, Chu2019b, Peake2020}.  In a study 
by \cite{Lin2019},  a DRL framework is designed to control a CACC AV platoon.  The 
DRL framework uses the Deep Deterministic Policy Gradient (DDPG) \cite{Lillicrap2016} 
algorithm and is found to have near-optimal performance \cite{Lin2019}.  In addition, 
\cite{Peake2020} identifies limitations in platooning with regard to the communication.  
Through the application of a multi-agent reinforcement learning process, i.e. a policy 
gradient RL and LSTM network, the performance of a platoon containing 3-5 vehicles was 
improved upon that of current RL applications to platooning \cite{Peake2020}. Furthermore, 
Model Predictive Control (MPC) is the current state-of-the-art for real-time optimal 
control practices \cite{Lin_2021}.  The study performed by \cite{Lin_2021} applies both 
MPC and DRL methodologies to the AV platoon problem, observing a DRL model using the 
DDPG algorithm to have a 5.8\% episodic cost higher than the current state-of-the-art. 
The works of \cite{yan2021hybrid} propose a hybrid approach to the AV platooning 
problem where the platoon is modeled as a Markov Decision Process (MDP) in order to 
collect two rewards from the system at each time step simultaneously.  This approach 
also incorporates jerk, the rate of change of acceleration in the calculation of the 
reward for each vehicle in order to ensure passenger comfort \cite{yan2021hybrid}.  The 
hybrid strategy led to increased performance to that of the base DDPG algorithm, as the 
proposed framework switches between using classic CACC modeling and DDPG depending on 
the performance degradation of the DDPG algorithm \cite{yan2021hybrid}. In another 
study by \cite{Zhu2019}, a DRL model is formulated and trained using DDPG to be evaluated 
against real world driving data. Parameters such as time to collision, headway, and jerk 
were considered in the DRL model's reward function.  The DDPG algorithm provided favourable 
performance to that of the analysed human driving data \cite{Zhu2019}, with favourable 
time to collision results, more efficient driving via reduced vehicle headways, and 
improved passenger comfort with lower magnitudes of jerk.

When formulating the AV platooning problem as a DRL model DDPG is prominently selected 
as the algorithm for training.  DDPG's ability to handle continuous actions space and 
complex state's is perfect for the CACC platoon problem.  However, despite the DDPG 
algorithm's success in literature, there are still instability challenges related to 
the algorithm along with a time consuming hyper-parameter tuning process to account 
for the minute differences in vehicle models/dynamics amongst platoons. As previously 
discussed, FRL provides advantages in these areas where information sharing can accelerate 
performance during training and improve the performance of the system as a whole. In 
addition, the ability to share experience across like models has been proven to allow 
for fast convergence of models \cite{Lim2020}, which further optimizes the performance 
of DDPG when applied to AV platoons.