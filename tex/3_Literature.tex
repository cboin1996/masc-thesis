\chapter{Literature Review} \label{chap:litreview}
In this chapter, an in-depth summary of the state-of-the-art research related to
FRL and DRL applied to AV platooning is presented.

\section{Federated Reinforcement Learning}
Although FL provides promise for supervised learning tasks, FL has been recognized to have
shortcomings in dynamic environments \cite{Li2020a}.   In addition, the works of FL
are largely associated with supervised learning tasks. As opposed to supervised learning,
RL is a sequential learning process where data is often non-IID with a small sample space
\cite{Zhuo2019, sutton2018reinforcement}. FRL has been developed and explored to enable FL advantages
in RL settings.

As previously discussed in Chapter \ref{chap:background}, FRL can be categorized into two categories: HFRL and VFRL.
HFRL provides the ability to aggregate experience while increasing the sample efficiency thus
providing more accurate and stable learning  \cite{IntelAI19}.
It is important to note that few existing works apply
FRL to autonomous driving at the time of this study.  Some of the current works studying
HFRL in a variety of applications are summarized below.

% HFRL
% HFRL references here %
A study by \cite{Lim2020} aims to increase the performance of RL methods applied to
multi-internet-of-things (multi-IoT) device systems.  RL models trained on single devices are often unable to control
devices in a similar albeit slightly different environment \cite{Lim2020}.  Currently,
multiple devices need to be trained using separate RL agents \cite{Lim2020}. The methods
proposed by \cite{Lim2020} sped up the learning process by 1.5 times for a two agent system.
In a study by \cite{Nadiger2019}, the challenges in the personalization of dialogue
managers, smart assistants and more are explored.  RL has proven to be successful in
practice for personalized experiences; however long learning times and no sharing of data
limits the ability for RL to be applied at scale.  Applying HFRL to atari non-playable
characters in pong showed a median improvement of ~17\% for the personalization time
\cite{Nadiger2019}. Furthermore, \cite{Liu2019b} discusses RL as a promising algorithm for
smart navigation systems, with the following challenges: long training times, poor
generalization across environments, and storing data over long periods of time.  In order
to address these problems \cite{Liu2019b} proposed the architecture `Lifelong FRL',
which can be categorized as an HFRL problem.  It was found the Lifelong FRL increased
the learning rate for a smart navigation system when tested on robots in a cloud
robotic system \cite{Liu2019b}. Finally, \cite{Ren2019} utilizes FRL to train multiple distributed
DRL agents for an IoT energy harvesting application.  It was found that FRL
provided advantages to DRL when applied to the same system model \cite{Ren2019}.


% VFRL
As opposed to HFRL, VFRL's environment structure is composed of multiple agents exploring
a single global environment. Some existing works on VFRL are summarized in this paragraph.
A study by \cite{Zhuo2019} identifies the challenge of building high quality Q-network policies in
small feature spaces or with limited training data.  In order to address this problem, \cite{Zhuo2019} applies FRL
by sharing encrypted Q-network outputs amongst agents while building a
shared value network which uses each agent's encrypted outputs as inputs.
\cite{Zhuo2019} demonstrates the effectiveness of VFRL in training high-quality policies
even with no training data shared amongst agents.
The works of \cite{Wang2018a} applies current DRL techniques with a custom FL framework to a mobile edge computing scenario
aiming to optimize caching and communication. \cite{Wang2018a} compares a custom FRL
framework against a centralized double-DQN optimal model, and finds
that FRL provides near optimal performance.  Furthermore, \cite{Wang2020a} proposes a framework
`Favor' designed to select which agent(s) in a distributed
training process may contribute to each FRL training round. It is concluded that `Favor' can reduce
the number of communication rounds in FRL by up to 49\%, 23\% and 42 \% on the MNIST, FashionMNIST, and CIFAR-10
datasets, respectively \cite{Wang2020a}. Another study by \cite{Liang2019} proposes a federated transfer
RL process (FTRL) atop of a DDPG implementation designed to control steering angle of AVs.
The FTRL process includes three steps: online
transfer step, standard DRL step and lastly a FedAvg step \cite{Liang2019}.
It is concluded that applying FTRL in both a simulated and real environment led to increased
performance when compared to the base DDPG algorithm \cite{Liang2019}.  Table \ref{tab:HFRLVFRLworks} provides a summary of existing
works for HFRL and VFRL.


% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Summary of Existing Works for HFRL and VFRL Architectures}
    \begin{tabular}{ll} \toprule
    \textbf{FRL Architecture} & \textbf{References} \\ \midrule
    \midrule
    HFRL & \cite{Lim2020}, \cite{Nadiger2019}, \cite{Liu2019b}, \cite{Ren2019} \\
    VFRL & \cite{Zhuo2019}, \cite{Wang2018a}, \cite{Wang2020a}, \cite{Liang2019} \\ \bottomrule
    \end{tabular}%
  \label{tab:HFRLVFRLworks}%
\end{table}%

It is clear that both HFRL and VFRL have promising research directions, however
the AV platooning problem best identifies with HFRL due to the following reasons:

\begin{itemize}
  \item Training a decentralized platoon follows that of an HFRL
  training processes: (train locally, upload to server, aggregate parameters, update
  each agent with aggregated parameters)
  \item An AV platoon is composed of vehicles, or agents. Each agent's environment may
  differ slightly from each other, and are isolated with the exception of some shared state
  data such as the acceleration of the predecessor vehicle.
\end{itemize}

In addition, the successes of the FedAvg algorithm as a means to improve performance and training
times for systems has inspired further research into how aggregation methods should
be applied.  The design of the aggregation method is crucial in providing performance
benefits to that of the base case where FRL is not applied.  The FedAvg
\cite{BrendanMcMahan2017a} algorithm proposed the averaging of gradients in the
aggregation method.  In contrast, \cite{Liang2019} proposed using model weights in
the aggregation method for AV steering control. Thus, FRL applications can differ
based upon the selection of which parameter to use in the aggregation method.  A study
by \cite{ZhangX2020} explores applying FRL to a decentralized DRL system optimizing
cellular vehicle-to-everything (V2X) communication. \cite{ZhangX2020} utilizes the model
weights in the aggregation method, and describes a weighting factor dividing the sum
batch size for all agents by the training batch size for a specific agent.  In addition,
the works of \cite{LimHyun2021} explores how FRL using gradient aggregation can improve
convergence speed and performance on the OpenAI-gym environments CartPole-V0,
MountainvehicleContinuous-V0, Pendulum-V0 and Acrobot-V1.  \cite{LimHyun2021} determines
that aggregating gradients using FRL creates high performing agents for each of the
OpenAI-gym environments relative to models trained without FRL.  In addition,
\cite{WangXiaofei2021} applies FRL to heterogeneous edge caching.  \cite{WangXiaofei2021}
shows the effectiveness of FRL using weight aggregation to improve hit rate, reduce
average delays in the network and offload traffic. Lastly, \cite{Huang2021} applies FRL
using model weight aggregation to service function chains in network function virtualization
enabled networks. \cite{Huang2021} observes that FRL using model weight aggregation
provides benefits to convergence speed, average reward and average resource consumption.
Below is a summary of the previously mentioned FRL works categorized by their proposed
aggregation method.

% Table generated by Excel2LaTeX from sheet 'Sheet1'
\begin{table}[H]
  \centering
  \caption{Summary of Existing Works in the Area of FRL Categorized by FRL Aggregation Method}
    \begin{tabular}{ll} \toprule
    \textbf{FRL Aggregation Strategies} & \textbf{References} \\ \midrule
    \midrule
    Gradient Averaging & \cite{BrendanMcMahan2017a}, \cite{LimHyun2021} \\
    Model Weight Averaging & \cite{Liang2019}, \cite{ZhangX2020}, \cite{WangXiaofei2021}, \cite{Huang2021} \\ \bottomrule
    \end{tabular}%
  \label{tab:addlabel}%
\end{table}%


Despite the differences in FRL applications within the aforementioned studies, each study
maintain a similar goal: to improve the performance of each agent within the system.  None
of the aforementioned works explore the differences in whether gradient or model weight
aggregation is favourable in performance, and many of the works apply FRL to distributed
network or communications environments.  It is the goal of this study to conclude whether
model weight or gradient aggregation is favourable for AV platooning, as well as be one of
the first (if not the first) to apply FRL to AV platooning successfully.

\section{Deep Reinforcement Learning applied to AV Platooning} \label{sec:avRL}

In order to improve the limitations of vehicle following models, DRL has been a
steady area of research in the AV community with many authors contributing works
to DRL applied to CACC \cite{Schwarting2018, Li2019, Kendall2019, zhangYuxiang2020}.
By assuming vehicle-vehicle (V2V) or V2X communications, it is possible to model AV platooning scenarios
where key driving data is shared amongst vehicles in a platoon \cite{Schwarting2018,Hussain2019}.
The sharing of data amongst vehicles has enabled successes for CACC in literature as seen in \cite{LeiV2x}.
As previously stated, classically controlled AV platoons have limitations related to
accuracy, poor ability for models to generalize, and a lack of adaptive updating in the models \cite{Zhu2018}.
As a result of such limitations, DRL applied to AV platooning has become a large
research ecosystem for authors to explore \cite{Vinitsky2018, Kiran2020}.

Many of the existing AV platooning works apply DDPG to AV platooning environments.
Due to the popularity of the DDPG algorithm in AV platooning, DDPG has been selected
as the algorithm of choice in this thesis.
In addition, CACC models are often selected as the model of choice in AV platooning, and as such
a CACC model is proposed for the design of the platooning environment in this thesis.
Some studies closely related to the works of this thesis are summarized below.

In a study
by \cite{Lin2019},  a DRL framework is designed to control a CACC AV platoon.  The
DRL framework uses the DDPG \cite{Lillicrap2016}
algorithm and is found to have near-optimal performance \cite{Lin2019}.  In addition,
\cite{Peake2020} identifies limitations in platooning with regard to the communication.
Through the application of a multi-agent reinforcement learning process, i.e. a policy
gradient RL and LSTM network, the performance of a platoon containing 3-5 vehicles was
improved upon that of current RL applications to platooning \cite{Peake2020}. Furthermore,
model predictive control (MPC) is the current state-of-the-art for real-time optimal
control practices \cite{Lin_2021}.  The study performed by \cite{Lin_2021} applies both
MPC and DRL methodologies to the AV platoon problem, observing a DRL model using the
DDPG algorithm to have a 5.8\% episodic cost higher than the current state-of-the-art.
The works of \cite{yan2021hybrid} propose a hybrid approach to the AV platooning
problem where the platoon is modeled as a markov decision process (MDP) in order to
collect two rewards from the system at each time step simultaneously.  This approach
also incorporates jerk, the rate of change of acceleration in the calculation of the
reward for each vehicle in order to ensure passenger comfort \cite{yan2021hybrid}.  The
hybrid strategy led to increased performance to that of the base DDPG algorithm, as the
proposed framework switches between using classic CACC modeling and DDPG depending on
the performance degradation of the DDPG algorithm \cite{yan2021hybrid}. In another
study by \cite{Zhu2019}, a DRL model is formulated and trained using DDPG to be evaluated
against real world driving data. Parameters such as time to collision, headway, and jerk
were considered in the DRL model's reward function.  The DDPG algorithm provided favourable
performance to that of the analysed human driving data \cite{Zhu2019}, with favourable
time to collision results, more efficient driving via reduced vehicle headways, and
improved passenger comfort with lower magnitudes of jerk. As V2X
communications are envisioned to have a beneficial impact on the performance of platoon
controllers, the works of \cite{LeiV2x} investigates the value of V2X communications for
DRL-based platoon controllers. \cite{LeiV2x} emphasizes the trade-off between the gain of
including exogenous information in the system state for reducing uncertainty and the
performance erosion due to the curse-of-dimensionality. Lastly, \cite{Yuan2019}
presents the introduction of vehicle dynamics into an adaptive cruise control model.
Using a custom DRL model, trained by DDPG, \cite{Yuan2019} finds that the DRL model is optimized
to a performance level near that of dynamic programming methods.

When formulating the AV platooning problem as a DRL model DDPG is prominently selected
as the algorithm for training.  DDPG's ability to handle continuous action spaces and
complex states is perfect for the CACC platoon problem.  However, despite the DDPG
algorithm's success in literature, there are still instability challenges related to
the algorithm along with a time consuming hyper-parameter tuning process to account
for the minute differences in vehicle models/dynamics amongst platoons. As previously
discussed, FRL provides advantages in these areas where information sharing can accelerate
performance during training and improve the performance of the system as a whole. In
addition, the ability to share experience across like models has been proven to allow
for fast convergence of models \cite{Lim2020}, which further optimizes the performance
of DDPG when applied to AV platoons.